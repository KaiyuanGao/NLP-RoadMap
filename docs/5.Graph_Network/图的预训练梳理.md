### 1、节点级别的预训练

使用易得的无标签数据捕获图中特定领域的知识信息，两种：**上下文预测**(context prediction)和**属性遮掩**(attribute masking)

![image-20210112121737282](/Users/gaokaiyuan/Library/Application Support/typora-user-images/image-20210112121737282.png)

#### 属性遮掩

- 具体可以mask 节点/mask边，目的是通过学习图结构上节点/边属性分布的规律，捕获到领域知识。

- domain站间模型 使用的。
- 参考论文：Graph-Bert，GPT-GNN、G5

#### 上下文预测

在上下文预测中，使用子图预测其周围的图结构，目标是训练出一个GNN可以将有着相似上下文结构的节点映射成相近的表示向量。

- 邻居：节点v 的K-HOP邻居包含和v距离不超过K的所有节点和边
- 上下文图：v邻居的图结构，一般用r1, r2两个参数描述 
- 上下文锚点：一般 r1< K，这样**邻居和上下文图**可以**共享一部分节点**，将这些节点看做**上下文锚节点(context anchor nodes)**。这些锚节点描述了邻居和上下文图是如何相互连接的。

使用**负采样**联合学习**main GNN**和**context GNN**。main GNN将邻居编码成节点嵌入，context GNN将上下文图编码成上下文的嵌入。上下文预测的目标函数是一个**二分类问题**，**判断**特定的**邻居和上下文图是否属于同一个节点**



### 2、图级别的预训练

- 两大类：（1）对整图分类进行预测(有监督的)；（2）对图结构进行预测

- 参考论文：GCC、L2P-GNN



## 3、注意点

#### 预训练与微调两个阶段的gap问题

- 实验：预训练+微调 效果 不如 直接训练
- 原因：预训练力度过大，导致太偏向预训练任务；微调任务太简单
- 可能的解决：数据足够，直接重训练；数据不够，取预训练偏底层的向量；

#### 图网络的过平滑问题

![image-20210114110015137](/Users/gaokaiyuan/Library/Application Support/typora-user-images/image-20210114110015137.png)