关注图结构信息的GNN预训练工作，

- 论文：[Pre-Training Graph Neural Networks for Generic Structural Feature Extraction](https://arxiv.org/pdf/1905.13728.pdf)
- 代码：未开源

主要研究了以下两个问题：

- 图上的预训练能够为学习图信息带来增益？
- 如何设置合理的预训练任务？

### 为什么要预训练

简单说，① 大量且高质的标注数据难以获得；② 预训练在NLP、CV 效果增益明显，可以充分探索数据

### 预训练框架

整个预训练框架如下：

![GQeWwADK49hkPoJ](https://i.loli.net/2021/04/27/GQeWwADK49hkPoJ.png)

#### step-1：输入

主要是一些图结构特征：

- Degree：节点的度，表示节点的local importance；
- Core Number：表示节点的联通性；如果一个点的核数为K，表示存在一个子图包含该点，子图中点度数的最小值是K
- Collective Influence：节点的邻居的重要程度
- Local Clustering Coefficient：节点一跳邻居的联通性

将上述特征`最大最小归一化`之后作为GNN的输入（除Local Clustering Coefficient不需要归一）

具体计算方法：

![edk7WFnPwtYUayN](https://i.loli.net/2021/04/27/edk7WFnPwtYUayN.png)

#### step-2：预训练任务

##### 任务一：边重建（ Denoising Link Reconstruction）

思路就是随机mask掉边，然后通过一个解码器来判断两个节点之间是否存在边链接。

通过边重建任务，预训练的GNN能够学习到节点embedding的一种较为鲁棒的表示，这种表示在含有噪声或者边信息部分丢失的图数据中很有效。

##### 任务二：中心打分排序（Centrality Score Ranking）

Centrality Score 可以认为是节点在整个图中的重要性度量，该任务就是对节点根据重要性进行排序。主要使用了四种`Centrality Score`：

- Eigencentrality：从高分节点对邻居贡献更多的角度衡量节点的影响
- Betweenness：某节点位于其他节点之间的最短路径上的次数
- Closeness：某节点与其他节点间的最短路径的总长度
- Subgraph Centrality：某节点对所有子图的参与度(到所有子图最近路径长度的和)

最终预训练学习的目标就是，判断每种打分两个节点之间的序关系。通过Centrality Score Ranking任务，预训练的GNN能够学习到图中的每一个节点在全局中起到的作用。

##### 任务三：图簇信息（Cluster Preserving）

假设一个大图可以不重叠地划分成K个子簇，并且存在指示函数 $I_{c}(·)$ 来告知给定节点是否属于簇 C【这个指示函数可以通过程序的算法得到，比如联通子图算法。】

然后该任务就是根据GNN得到的node-emb，和cluster-emb 之间的距离是否足够近。

cluster-emb通过attention来获得：
$$
\mathcal{A}\left(\left\{\mathcal{F}^{\text {cluster }}\left(\mathcal{G}^{*}\right)[v] \mid v \in C\right\}\right) \text { . }
$$

### 实验

![Ot4EFCSaiecRPfu](https://i.loli.net/2021/04/27/Ot4EFCSaiecRPfu.png)

![image-20210427163222492](/Users/gaokaiyuan/Library/Application Support/typora-user-images/image-20210427163222492.png)

### 思考

- 这里GNN输入用的一些图结构统计信息，之前没有尝试过。但是对于大规模图来说，计算统计特征可能有些复杂。
- 第三个预训练任务在实际数据上不是很合理（论文中是虚构的图数据）
- 第一个预训练任务 mask掉边 可能会影响第二个预训练任务的效果？