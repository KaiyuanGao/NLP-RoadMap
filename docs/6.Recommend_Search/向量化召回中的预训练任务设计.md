来自论文：PRE-TRAINING TASKS FOR EMBEDDING-BASED LARGE-SCALE RETRIEVAL

### 模型

双塔表示，计算相似度

![image-20210408131110887](/Users/gaokaiyuan/Library/Application Support/typora-user-images/image-20210408131110887.png)

主要贡献在于提出了三种预训练任务

### 预训练任务

![image-20210408131218062](/Users/gaokaiyuan/Library/Application Support/typora-user-images/image-20210408131218062.png)

- Inverse Cloze Task (ICT) ：和word2vec比较类似，不过元素变成句子。从一个doc中随机sample一个句子，然后希望这个句子和整个doc的embedding是接近的。
- Body First Selection (BFS) ：假设了**每篇wiki的第一段是可以表达整篇文章整体含义的**。从第一段随机sample一个句子，再从这个页面其他地方随机sample一个段落d，希望他们的embedding是接近的
- Wiki Link Prediction (WLP)：假设了**每篇wiki的第一段是可以表达整篇文章整体含义的**。从第一段随机sample一个句子，再从这个页面的超链中随机sample一个wiki的某一段落d，希望他们的embedding是接近的。
- Masked LM (MLM)：

注意这里的训练任务跟NSP不一样，没有额外构造负样本。全是正样本，相当于**构造了海量弱标签数据让双塔模型更好的学习**



> 感觉不是很靠谱的样子。。。