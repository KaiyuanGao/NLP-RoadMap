主要是[SEBASTIAN RUDER](https://ruder.io/author/sebastian/index.html)大神的博文 [Recent Advances in Language Model Fine-tuning](https://ruder.io/recent-advances-lm-fine-tuning/) 的阅读笔记，以及个人项目经验复盘。

#### 预训练微调标准范式

首先大规模无标注数据上预训练，然后在有标注数据上针对特定下游任务微调。

![img](https://ruder.io/content/images/2021/02/pretraining_finetuning.png)

除此之外，最近出现一系列微调优化模式，整理在下面

![img](https://ruder.io/content/images/2021/02/fine-tuning_methods_overview.png)



#### Adaptive FT

主要是为了解决预训练与微调阶段数据Gap问题（主要是domain和task不一致导致），从原本的两阶段范式，改进成了三阶段。具体而言：

1. 大规模通用预训练，无标注数据
2. 中等规模任务相关预训练，无标注数据

3. 下游特定任务微调，有标注数据

![img](https://ruder.io/content/images/2021/02/adaptive_fine-tuning-1.png)

一些参考：

- [Zero-Shot Entity Linking by Reading Entity Descriptions](https://www.aclweb.org/anthology/P19-1335.pdf)
- [Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling](https://www.aclweb.org/anthology/D19-1433.pdf)
- [Pretraining Methods for Dialog Context Representation Learning](https://www.aclweb.org/anthology/P19-1373.pdf)
- [Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks](https://www.aclweb.org/anthology/2020.acl-main.740.pdf)
- [MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer](https://www.aclweb.org/anthology/2020.emnlp-main.617.pdf)

#### Behavioural FT

