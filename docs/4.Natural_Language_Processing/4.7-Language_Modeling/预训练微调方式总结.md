> 主要是[SEBASTIAN RUDER](https://ruder.io/author/sebastian/index.html)大神的博文 [Recent Advances in Language Model Fine-tuning](https://ruder.io/recent-advances-lm-fine-tuning/) 的阅读笔记，以及个人项目经验复盘。

#### 预训练微调标准范式

首先大规模无标注数据上预训练，然后在有标注数据上针对特定下游任务微调。

![img](https://ruder.io/content/images/2021/02/pretraining_finetuning.png)

除此之外，最近出现一系列微调优化模式，整理在下面

![img](https://ruder.io/content/images/2021/02/fine-tuning_methods_overview.png)



#### Adaptive FT

主要是为了解决预训练与微调阶段数据Gap问题（主要是domain不一致导致），从原本的两阶段范式，改进成了三阶段。具体而言：

1. 大规模通用预训练，无标注数据
2. 中等规模任务相关预训练，无标注数据

3. 下游特定任务微调，有标注数据

![img](https://ruder.io/content/images/2021/02/adaptive_fine-tuning-1.png)

一些参考：

- [Zero-Shot Entity Linking by Reading Entity Descriptions](https://www.aclweb.org/anthology/P19-1335.pdf)
- [Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling](https://www.aclweb.org/anthology/D19-1433.pdf)
- [Pretraining Methods for Dialog Context Representation Learning](https://www.aclweb.org/anthology/P19-1373.pdf)
- [Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks](https://www.aclweb.org/anthology/2020.acl-main.740.pdf)
- [MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer](https://www.aclweb.org/anthology/2020.emnlp-main.617.pdf)

#### Behavioural FT

同样是为了解决预训练和微调Gap问题，这里主要是Task-specified。也是三阶段，具体而言：

1. 大规模通用预训练，无标注数据
2. 与目标任务接近的训练任务，有标注无标注均可
3. 下游特定任务微调，有标注数据

![img](https://ruder.io/content/images/2021/02/behavioural_fine-tuning.png)

#### Parameter-efficient fine-tuning

常见的微调都会调整预训练模型的所有参数然后保存使用，如果任务场景很多的话，保存所有模型会比较浪费（computationally expensive）。于是就提出了，` keeping most of the model parameters fixed and fine-tuning a small number of parameters per task` 。

1. 具体而言，一种做法是`Adapter`，大致思想是将一个可插拔的小模块插入预训练模型中间，然后预训练模型的参数不动，调整这个小模块参数。不过这个还没仔细了解过，先留坑。`!!! TODO @!!`

![img](https://ruder.io/content/images/2021/02/modular_adapters.png)

2. 此外，另外一种思路是：直接修改预训练模型参数。微调结果可以认为是在通过特定任务来修改预训练模型参数：

$$
\theta_{\text {fine-tuned }}=\theta_{\text {pre-trained }}+\theta_{\text {task }}
$$

​	所以，我们可以直接保存一份预训练参数，然后每次替换 `task`的参数即可。或者也可以直接freeze掉预训练下层参数，调整上层参数。

3. 另外还有研究是：在微调过程裁剪掉预训练参数。记得有一篇 [论文](https://arxiv.org/pdf/1905.09418.pdf) 是讲在原始BERT中，多头（12个）并不是所有头都对微调任务有作用的，我们可以直接删除某些头来达到加速和优化。



#### Text-to-text fine-tuning



#### Mitigating fine-tuning instabilities

